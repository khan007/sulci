=======================
Notes sur le textmining
=======================

====================
Quelques définitions
====================

*Normalisation*
Nettoyer une chaîne des caractères non pertinents pour le sens.

*Lemmatisation*
Réduire un mot à son lemme, sa racine.

*Tokénisation, itémisation*
Découper un texte en unités lexicales élémentaires (mots).

*Classification, catégorisation*
Fait de donner des descripteurs à un texte.

*Information mutuelle*
L'information mutuelle compare la probabilité d'observer deux éléments ensemble avec la probabilité d'observer ses deux éléments séparément.
Si la probabilité de les observer ensemble est plus élevée, alors on a une collocation potentielle.

*Etiquettage*
L'étiquettage associe une catégorie syntaxique unique à chaque mot d'un texte. Différent de l'analyse morphologique, qui peut associer plusieurs catégories syntaxiques à un mot. L'étiquettage porte sur les mots, pas sur les syntagmes (groupes de mots). L'étiquettage doit résoudre les ambiguïtés d'étiquettage (un mot peut avoir plusieurs étiquettes ; il faut choisir la bonne dans le contexte considéré).

*Collocation*
Suite de mots considérés comme un même élément sémantique (dans un contexte donné). Ex. : chat noir, New York, avenue des Champs-Elysées...
En toute rigueur, le sens du tout ne correspond pas au sens de chacune des parties.

*Bigram*
Paire de mots d'un texte apparaissant côté à côte. Si la fréquence est élevée, le bigram est une collocation.

*Trigram*
Trois mots qui se suivent dans un texte. Diviser en trigrams un texte.

*Désambiguïsation*
Fait de définir le sens d'un mot plurysémique dans un contexte donné.

*Corpus*
Ensemble de textes servant de référence pour les outils d'analyse. Ces textes peuvent être plus ou moins catégorisés.


=====
Algos
=====

Collocations
------------
Doit-on chercher les collocations sur les lemmes ou sur les graphies complètes ?
Conseil français du culte musulman ( CFCM => ) non prises puisque non meanings... what to do with parenthesis ?


====
Vrac
====
approche statistique vs approche sémantique
Outil : découper en phrases.
Outil : découper en bigram
Outil : découper en trigrams
Objectif premier du text-mining : catégoriser les textes
Tenter de trouver les versions les plus complètes des items avant de les soumettre au thésaurus
Etre dans le thésaurus ne suffit pas, il faut être présent souvent : d'abord la statistique pour déterminer les candidats ; ensuite les validations, dont celle du thésaurus.
Comparer la fréquence d'un mot (d'une expression) dans un contexte donné avec sa fréquence dans le corpus.
problème du matching entre une expression et le thésaurus : graphie exacte, on passe à côté parfois ; graphie simplifiée (lemme...), on touche trop large parfois.
Récupérer le texte qui est dans des <a> ou des <strong> pour pondérer par la suite des collocations ?
Quelle forme utiliser pour le matching ?
keyphrase
tant qu'y a qu'un item d'une collocation qui apparaît souvent seul, ça va ; c'est quand y en a beaucoup que ça fout le bordel.
Par exemple, le fait que "Sarkozy" apparaissent 40 fois tout seul dans un texte ne remet pas du tout en question le fait que "Nicolas Sarkozy" est une collocation.
Pire, si Nicolas apparaît aussi tout seul, ça ne le remet pas forcément en question. Ah on est bien, là, hein?
On peut imaginer que les ngrams qui sont considérés comme des noms propres avec une forte probabilité ne soient pas soumis au PMI.
Il faut avoir confiance dans sa lemmatisation, sinon si la lemmatisation a une trop grande marge d'erreur (notamment en faisant que deux termes différents sont lemmetisés de la même manière), c'est la merde. C'est là que le corpus peut aider pour la désambiguïsation (ne pas mélanger un verbe et un nom, par exemple).
Piste de syntaxe de corpus :
La!#/t=at souris!#/t=nm,kw=387 mang!#/t=vb le!#/t=at chat!#/t=nm,kw=469.
Où on définit pour chaque élément sa fonction syntaxique (t), dans le contexte, et un éventuel lien vers un key word du thésaurus.
Le tout en mode lemmatisé.
Une petite interface JS pour administrer ça, et le tour est joué...
Faut-il considérer les textes séparément dans le corpus (+ galère semble-t-il), ou considérer le corpus comme une liste de phrases (éventuellement qui se suive, pour les antécédents et Cie).
Ça pas beau : culturisme => (u'Culture',) in thesaurus
Chercher dans le thésaurus via versions lemmatisées, et s'il y a plusieurs concordances, trouver la plus proche via levenshtein (en comparant les graphies cette fois, pas les lemmes)
Peut-on dire que si un seul des items pertinents d'un ngram n'apparaît jamais en dehors, alors la collocation est validée ?
Dans le corpus, taguer des textes pour pouvoir ensuite aider à la pondération de la confiance dans un kw si les mêmes termes revienne (en plus du kw)
Utiliser la titraille pour surpondérer (si un terme est dans la titraille, il pèse plus)?
Eglise catholique vs Église catholique => là encore, il faut trouver la "bonne" standardisation pour maximiser le matching.
Ou bien, comme pour le thésaurus, faire un matching très tolérant, validé ensuite par une distance de levenshtein.
Ça pas beau (de Vieille Ville, à Jerusalem): (u'Vieille',) in thesaurus => vieillissement
"merger" une collocation avec une collocation plus grande si et seulement si son score est égal ou plus élevé
Les inter, ça fout la merde ("Main tendue" donne du poids à "main tendue" dans le texte...).
A partir de quel pourcentage de fréquence on considère un ngram (ou un mot) comme candidat ?
Ajouter un form_confidence, qui teste si on a affaire à une date, un chiffre, etc.
Peut-on imaginer que quand un mot-clé proposé pour un texte est refusé, alors le texte soit ajouté dans le corpus avec l'information qu'il ne comporte pas le concept en question ?
Rendre toujours disponible en ligne le thésaurus de Libération, pour que les gens puissent l'utiliser. De même pour le corpus ?

Suggestion de noms pour le projet (from Django's songs): abc, cloods, echoes, nuages, parfum, reflets, tel-quel

la collocation essaie de savoir si la statistique a du sens ; le corpus, si le sens a du sens dans ce contexte.
Ne pas stemmer quand le mot commence par une majuscule ?
Etape : devenir keyphrase, puis de venir keyphrase qui a du sens, puis devenir keyphrase qui a le plus de sens dans le contexte
"bourreau" est dans le thésaurus, mais n'est pourtant pas pertinent pour un texte sur "Jérôme Bourreau"

pôle innovation is duplicate pôle innovation web
Comparing 'pôle innovation' and 'pôle innovation web'
MI : 3.000000 / 2.000000
pôle innovation web will be deleted
=> voir si "pôle innovation" apparaît sans "web", si non, y un sushi dans l'algo

Objectif numéro 1 : ne passer à côté d'aucune information présente.

l'Église catholique (29.629630) => remove "l'" if in first position.

Appliquer des "templates" aux ngrams : nom adjectif, nom nom adjectif, etc. ?

Un cas chiant : "le maire de Paris Bertrand Delanoë", dans le cas où ces trois termes ("Paris", "Bertrand" et "Delanoë") apparaissent toujours ensemble et plus d'une fois.
Sans corpus, ça me paraît assez indémerdable.

Considérer certains mots comme stop_words seulement s'ils sont "seuls dans leur contexte" ? Genre "ensuite", "comme", etc.

Essai : si un stemm n'a qu'une occurrence, que cette occurrence est en position 1 dans sa phrase, et que le mot en position 2 n'est pas istitle, alors le istitle de ce stemm doit être false.
oui mais => "Pourquoi Panafieu ?" (Vérifier aussi que ces deux termes ne sont pas une collocation par ailleurs ?)

c'est pas parce qu'un stem ou ngram est unclus dans un autre que l'un doit forcément être mergé dans l'autre : sécurité, Conseil de sécurité

inclure une recherche 1 pour 1 dans le thésaurus pour la pondération, et notamment la déduplication ?

So Others Might Eat, qui n'apparaît qu'une fois dans le texte, y compris ses dérivés, finit en première position, c'est pas bueno.

Has interest == count supérieur à la moyenne ou is_title, et pas stop word

Prendre en compte le paragraphe, pour donner plus de poids au premier et au dernier ?

Première utilisation du corpus (si volumineux) : comparer la fréquence de chaque terme dans le texte courant avec sa fréquence dans le corpus

Redit : stocker les compteurs seulement, genre lemme_count = 234, voire lemme_frequency, mais ça on peut la calculer à la volée Ex. mang_count == blabla
Humm, y a aussi la fréquence moyenne de ce mot dans un texte où il apparaît et où il a été défini comme descripteur (par la doc, par exemple)

En fait, il faut définir deux score :
1. A quel point on pense qu'on a affaire à une entité (collocation)
2. A quel point on pense que cette entité est une entité clé du texte

Un défaut des algorithmes statistiques : l’inertie lorsqu’un événement change la signification des mots employés. Un document du 20 mai mentionnant le « président de la république » pourrait facilement être classé avec ceux parlant de J. Chirac si le corpus passé est important.

Quid des relations entre entités ?

Regarder dans le corpus des articles de Libé tous ceux qui sont tagués "xxx" et extraire les termes les plus courants de ces textes mis en commun
Si on prend que les recouvrements, on doit pouvoir extraire une matrice, et comparer ensuite le texte courant avec cette matrice
Faut pas avoir à boucler sur toutes les matrices, donc faut peut-être indexer les matrices selon leur termes, et ensuite pour chacun des termes du texte courant, voir quelle matrices correspondent, et comparer la matrice du texte courant seulement avec celles-ci

But ultime : déceler le sens implicite, i.e. qui n'est pas écrit texto

Title : Un mot est "pauvre" s'il répond à des règles en faisant un mot courant (genre adverbe), et que dans le contexte présent ce n'est pas un title

Le text-mining, ce n'est pas seulement poser des descripteurs issus d'une liste d'autorité, parfois un verbe plus complément peut en dire autant sur le texte.

calculer la distance entre les keyentities ?

thesaurus confidence : distance de lev avec le résultat le plus pertinent si y a

garder la ponctuation finale des phrases ? Par exemple pour déterminer la tonalité d'une phrase, et donc la tonalité d'un texte.
La garder aussi en début de phrase suivante, pour déterminer si le mot doit commencer ou non par une cape ?


La "lemmatisation" (ou tentative de réduction d'un mot à sa partie la plus essentielle et représentative) devrait prendre en compte le contexte : ne pas enlever la cape si on n'est pas à la première place de la phrase, pour donner un exemple grossier
Tout l'enjeu est notamment de ne pas avoir une lemmatisation trop feroce : que "journaliste" et "Journalists" dans "International Consortium of Investigative Journalists" soient reduits au même lemme ne fait pas sens

Arf, problème des abréviations, où les points ne sont pas des séparateurs de phrases...

La confusion des racines est un problème qui survient lorsque l’algorithme de lemmatisation groupe des mots de nature différente sous une même racine.

Les flexions sont les différentes formes fléchies d’un même mot. Les formes fléchies correspondent aux formes « conjuguées » ou « accordées » d’un mot de base non conjugué et non accordé : le Lemme.

Deux étapes importantes : lemmatisation au lieu de stemmatisation (radicalisation) ; récupération des déclencheurs, en croisant les textes où un descripteur est posé.
tagger la fonction grammaticale puis lemmatiser

POS == Part-of-Speech == grammatical tagging

Créer des règles par défaut (mmment == adverbe, le, la == déterminant, aient == verbe, etc.) pour déterminer un tag par défaut
Puis des règles de vérification : déterminant + verbe est douteux, déterminant + nom est pertinent, etc.

Les templates de règles : 
Le mot précédent (suivant) est tagué xxx
Le mot précédent le précédent (suivant le suivant) est tagué xxx
Un des deux mots précédents (suivants) est tagué xxx
Le mot précédent est tagué xx et le mot suivant est tagué zz
Le mot précédent (suivant) est tagué xx et le mot précédent le précédent (suivant le suivant) est tagué zz

Hypothèse de Markov : un tag dépend seulement d'un nombre LIMITÉ de tags précédents

Voir l'ago Viterbi

le principal problème que doit résoudre l’ordinateur : l’ambiguïté

Une centaine de textes pour entraîner Brill

is_ambiguous = True / False

Le plus grand avantage des catégoriseurs neuronaux est qu’ils ne nécessitent pas de très gros corpus d’entraînement. Marques et Lopes 1996 rapportent une efficacité de 96 % pour le portugais pour un catégoriseur neuronal entraîné sur 30 pages de texte seulement. Toutefois, il est difficile de perfectionner les logiciels obtenus puisque les réseaux neuronaux ne nous donnent pas accès à des représentations symboliques, à des tableaux de statistiques ou encore à des règles, mais uniquement à un réseau. Bien comprendre toutes les interactions entre les divers éléments est donc extrêmement difficile.

Il y a une énorme quantité de noms propres, de mots étrangers, de mots rares, de mots de spécialité (termes), de sigles, de néologismes et de mots mal orthographiés qui ne peuvent pas tous se retrouver dans un corpus donné.

Les suffixes grammaticaux (genre, nombre, temps, personne et mode) ne modifient jamais la catégorie des mots. Donc, leur présence ou leur absence n’a pas d’impact direct sur la catégorie des mots. C’est pourquoi on peut donner à étudiant, étudiants, étudiante et étudiantes la même catégorie et il en est de même pour joue , jouerais , jouerai , etc. Les suffixes lexicaux quant à eux modifient la catégorie des mots auxquels ils s’adjoignent. Les mots formés à partir de certains d’entre eux sont ambigus (ex : -ique peut donner lieu à un nom ou à un adjectif) et d’autres non (ex. : -emment  : adverbe).

Un même suffixe lexical, donnera toujours lieu à des mots de mêmes catégories. Puisque les suffixes lexicaux modifient la catégorie des mots auxquels ils s’adjoignent, la catégorie de ceux-ci est prévisible à partir du suffixe. C’est-à-dire qu’un mot se terminant par -emment sera toujours un adverbe et un mot se terminant par -ique un nom ou un adjectif. On peut donc associer des catégories grammaticales aux suffixes lexicaux.

le catégoriseur de Brill fait tout de même une analyse linguistique d’une langue qu’il ne connaît pas et ne se base que sur les faits de surface

Fonctionnement de l’approche de Brill en bref :  il faut en entrée un corpus préalablement catégorisé. À partir de ce texte, un dictionnaire est créé ainsi que deux types de règles : contextuelles et lexicales. Les règles contextuelles vont servir à catégoriser les mots connus ambigus selon le contexte. Les règles lexicales vont servir à catégoriser les mots inconnus d’après leur morphologie et un contexte très restreint. Pour créer ces règles, le logiciel utilise une copie du corpus duquel les catégories ont été enlevées. Pour la création des règles contextuelles, les mots de ce corpus recevront dans un premier temps leur catégorie la plus fréquente tandis que pour la création des règles lexicales, les mots seront d’abord identifiés comme nom ou nom propre dépendant s’ils commencent par une majuscule ou une minuscule. Ensuite, le logiciel tente de corriger le résultat ainsi obtenu en créant des règles. Il compare sa catégorisation avec celle du corpus précatégorisé. À la première erreur trouvée, il crée plusieurs règles. Puis, il applique chacune de ces règles à tout le texte pour voir si les règles créées sont généralisables. La règle qui réduit le plus le nombre d’erreurs est enregistrée. Puis l’entraînement continu pour corriger la deuxième erreur.

Les règles sont tirées d'un corpus, elles sont donc relatives à ce corpus, changer de corpus change donc les règles.
Et quand on donne un score à une règle, ce score est relative à ce corpus.

Le logiciel détermine qu’il y a fin de phrase quand il rencontre un point (.), un point d’exclamation (!) ou un point d’interrogation (?) suivi d’une majuscule.

Proposer des suggestions si un mot ou un ngram n'est pas trouvé dans le thésaurus mais qu'un mot ou ngram proche est trouvé (voir code suggest)

Pour Brill il faut :
- un corpus catégorisé, dont :
    - une méthode pour créer un dictionnaire à partir du corpus
    - une méthode pour "prétagger" un texte qu'on voudrait ajouter dans le corpus (pour ne pas avoir à tous taguer à la main...
- un module d'apprentissage, dont :
    - une méthode qui crée le corpus de text (vierge de catégories)
    - une méthode qui applique les tags par défaut
    - une méthode qui compare le corpus de test avec le corpus référence pour extraire les erreurs
    - une méthode qui crée les règles depuis une erreur et sa correction
    - une méthode qui applique une liste de règles et détermine la plus pertinente
- un module d'utilisation, dont :
    - une méthode qui charge les règles
    - une méthode qui tag un mot dans un contexte donné en appliquant les règles
    - 

Il faut pouvoir entraîner le thérausus avec les articles passés de Libé avec une méthode approchant celle de Brill
Ex. d'algo :
Pour un descripteur donné, on prend tous les articles tagués avec ce descripteur
On en sort toutes les collocations : les déclencheurs
Pour chaque déclencheur, on l'applique à un jeu d'article de test, et on regarde si le déclencheur si trouve, si oui, on regarde si le descripteur est posé sur cet article ;
si non, on conclut que la liaison déclencheur-descripteur ne fonctionne pas tout seul.

Au moins checker que les tags d'un texte du corpus sont valides.

Un truc marrant à faire : les mots et expressions d'un auteur.

Ajouter une alerte si les rules.pdg et .rls ne sont pas similaires au moment du lancement des scripts.

Mettre les erreurs restantes à la fin (et donc le pourcentage d'erreur avec le nombre de tokens du corpus...).

troisième ligne de Perpignan Henri Tuilagi

Faire deux matchings dans le thésaurus : un texte intégral, un en essayant de maximiser si le premier n'a rien donné ?

Pour les stemm : ajouter un tag, mais ne pas prendre après les deux points ?

Extraire toutes les collocations, voir de quoi ils sont les déclencheurs et trier par descripteur en fonction du score de tous les déclencheurs liés et actifs

Nom d'un livre cité plusieurs fois dans un texte : Fabrique des filles , l' éducation des filles de Jules Ferry à la pilule => Obligé de mettre le max des ngrams à 15 (pk=705483)

=> recommencer automatiquement la boucle des ngrams si on a un ngram qui atteint la limite et apparaît plus de n fois ?

The most popular term weighting seems to be the inverse
document frequency, where the term frequency is weighted with respect to
the total number of times the term appears in the corpus. There is an exten-
sion of this designated the term frequency inverse document frequency
(tfidf )[46].The exact formulation for tfidf is given below
wi,j = tfi,j ∗ log(N/dfi),
where wi,j is the weight of term i in document j,tfi,j = number of occurrences
of term i in document j, N is the total number of documents, and dfi is the
number of documents containing term i.

ajouter une règle sur la position dans phrase ? Genre : il a été tagué x mais il aurait dû être tagué y parce que sa position est z (faire que avec la première, pour économiser des ressources lors de l'apprentissage ?)

ajouter tool pour checker les doublons dans lexique

Faire un outil qui liste les mots d'un texte par tag (avec un tag optionnel si on en veut qu'un)
Faire un outil qui permet de lister les tags d'un mot dans le corpus (pas le lexique du coup), plus pouvoir lister les occurrences d'un mot avec un tag donné
Penser à une option pour le case insensitive


=============
Lemmatisation
=============


Questions sur le prétagage :
Benoît XVI => CAR ?
ne pourront s'affirmer QU'avec la fin..., ne concerne QUE moi => SUB ??
pour prévenir TOUT incident lors de... => DTN ?
de FACTO ?
blabla TOUT en votant pour... ? => ADV ?
TANDIS que => PREP ?
il a appelé les JEUNES PALESTINIENS...
Cammas/SBP:sg n'/ADV est/ECJ:sg pas/ADV qu'/SUB un/DTN:sg monstre/SBC:sg => QU' ?
tout comme/SUB ?
N'/ADV importe/VCJ:sg quelle/ADJ:sg théorie/SBC:sg
si/ADV l'/PUL on/PRV:sg veut/VCJ:sg
des/DTC:pl jeunes/ADJ:pl hégéliens/SBC:pl
tout/ADV ce/DTN:sg à/PREP quoi/SBC:sg
à/PREP l'/DTN:sg état/SBC:sg naissant/VNCNT
Les/DTN:pl moins/SBC:pl de/PREP 25/CAR ans/SBC:pl
à/PREP plein/ADJ:sg de/PREP gens/SBC:pl => plein ?
Les/DTN:pl Nouveaux/SBP:pl Chemins/SBC:pl de/PREP la/DTN:sg connaissance/SBC:sg => Nouveaux ? (nom d'émission..)
4/CAR L/SBP:sg fourgonnette/ADJ:sg
Chuis/SBP:sg triste/SBC:sg
les/DTN:pl/le chaînes/SBC:pl/chaîne télé/SBC:sg
un/DTN:sg flash/SBC:sg d'/PREP/de agence/SBC:sg prometteur/ADJ:sg de/PREP très/ADV lourd/SBC:sg => prometteur ?
«/« Z'/SBP:sg allez/SBC:sg où/REL ?/? »/» 
que/SUB l'/DTN:sg/le on/PRV:sg trouve/VCJ:sg
n'/ADV/ne importe/SBC:sg quel/ADJ:sg journal/SBC:sg
y/PRV:++ compris/PAR:sg
via/PREP les/DTN:pl fleurs/SBC:sg
De/PREP:sg même/ADV ,/,
a/PUL priori/PUL
il/PRV:sg faut/VCJ:sg quand/SUB même/
quelque/ADV chose/SBC:sg
Je/PRV:sg/je ne/ADV l'/PRV:++ ai/ACJ:sg/avoir pas/ADV fait/PAR:sg ./.
la/DTN:sg réalité/SBC:sg même/ADJ:sg => même ?

attaché/PAR:sg (/( e/SBC:sg )/) s/SBC:sg
pousse/SBC:sg -/- toi-de/SBC:sg -/- là-que/SBC:sg -/- je-m'y-mette/SBC:sg
rendez-vous
etc.
le/DTN:sg 1er/SBC:sg
Qu'/REL alliez/SBP:sg -/- vous/PRV:pl
Qu'/SUB adviendra/VCJ:sg
ex/PUL -/- mouvement/SBC:sg

vérifier le tagage de "en"

Problème : quand on teste les scores des rules, on le fait avec le context à l'instant T, or, quand on applique la règle, plus ou moins de cas peuvent d'appliquer.
Par exemple si deux mots candidats se suivent, le fait d'appliquer la règle au premier peut avoir pour résultat que le deuxième terme ne sera plus candidat.

Mais ce n'est pas le cas pour la lemmatisation proprement dite, puisque les règles ne prennent pas en compte le contexte. On doit donc même pouvoir optimiser pour ne pas rechercher à corriger une erreur qu'on a déjà testé sans résultat...

J'ai un gros doute sur le fait de hiérarchiser les règles pour le lexical et lemmatizer... Ne vaut-il pas mieux TOUJOURS les traiter dans l'ordre de l'apprentissage ?

sortaient / portaient, galère galère

Lemmatisation (mots ramenés à leur forme canonique)
- substantifs ramenés au singulier
- adjectifs ramenés au masculin
- flexions d’un verbe ramenées à l’infinitif

Première apparition d'un ngram dans les archives de Libé (depuis 1994, donc...) ?

dans addsuf et Cie, doit-on tester que le mot trouvé dans le lexique a le même tag que le tag destination ?

mettre le nom du fichier dans -w

dans le nettoyage du texte, remplacer les retours-chariots sans ponctuation par un point ?

penser à (elle, lui, eux...)-même dans la tokenisation

Gaffe aux points comme mauvais séparateur de nombres

Les catégories par défaut utilisées pendant l'apprentissage doivent être les mêmes pour l'utilisation finale.

Un token qui n'a pas de veriified_tag doit renvoyer 0 dans les tests...

Le lexical trainer ne doit-il considérer que les mots qui ne sont pas dans le lexique => plutôt oui
Pour add tag : ne pas ajouter si le mot est dans le lexique et que le nouveau tag n'est pas dans les possibles ? Ça voudrait dire que, dès lors qu'un mot est dans le lexique, tous ses usages possibles doivent y être aussi...

dit-il/SBC:sg ./. 
va/ -/- t/PUL -/- elle
En/PREP Multi/SBP:sg 50,/SBC:sg Franck-Yves/SBP:sg Escoffier/SBP:sg
canoe.ca, Libération.fr => garder en un seul mot.
<p class="evenement">Godspeed You Black Emperor</p>\n<p>Retour => ajouter un point si retour chariot sans ponctuation avant.

Lexical rules : optimiser le tagging par défaut
Contextual rules : désambiguer

1. PREV --- previous(preceding)
2. PREVTAG --- preceding word is tagged
3. PREV1OR2TAG --- one of the two preceding words is tagged
4. PREV1OR2OR3TAG --- one of the three preceding words is tagged
5. WDAND2AFT --- the current word is x and the word two after is y
6. PREV1OR2WD --- one of the two preceding words is
7. NEXT1OR2TAG --- one of the two following words is tagged
8. NEXTTAG --- following word is tagged
9. NEXTWD --- following word is
10. WDNEXTTAG --- the current word is x and the following word is tagged z
11. SURROUNDTAG --- the preceding word is tagged x and the following word is tagged y
12. PREVBIGRAM --- the two preceding words are tagged
13. CURWD --- the current word is

#    @classmethod
#    def get_list(self):
#        return  ["PREVBIGRAM", #Le bigramme[a] précédent est X.
#                 "NEXTBIGRAM", #Le bigramme suivant est X.
#                 "NEXT1OR2OR3TAG", #1 des 3 mots suivants est catégorisé X.#Not in Inalf
#                 "NEXT1OR2TAG", #1 des 2 mots suivants est catégorisé X.
#                 "NEXT2TAG", #Le 2e mot suivant est catégorisé X.#Not in Inalf
#                 "NEXTTAG", #Le mot suivant est catégorisé X.
#                 "SURROUNDTAG", #Le mot précédent est catégorisé X et le mot suivant est catégorisé Y.
#                 "PREV1OR2OR3TAG", #1 des 3 mots précédents est catégorisé X.
#                 "PREV1OR2TAG", #1 des 2 mots précédents est catégorisé X.
#                 "PREVTAG", #Le mot précédent est catégorisé X.
#                 "PREV2TAG", #Le 2e mot précédent est catégorisé X.#Not in Inalf
#                 "NEXT1OR2WD", #1 des deux mots suivants est X.
#                 "NEXT2WD", #Le 2e mot suivant est X.
#                 "NEXTWD", #Le mot suivant est X.
#                 "CURWD", #Le mot est X.
#                 "PREV1OR2WD", #1 des deux mots précédents est X.
#                 "PREV2WD", #Le 2e mot précédent est X.
#                 "PREVWD", #Le mot précédent est X.
#                 "LBIGRAM", #Le mot précédent est X.#Not in Thibeault
#                 "RBIGRAM", #Le mot précédent est X.#Not in Thibeault
#                 "WDAND2BFR",#Not in Thibeault
#                 "WDAND2AFT",#Not in Thibeault
#                 "WDAND2TAGAFT", #Le 2e mot suivant est catégorisé X et le mot suivant est Y.
#                 "WDAND2TAGBFR", #Le 2e mot précédent est catégorisé X et le mot précédent est Y.
#                 "WDNEXTTAG", #Le mot suivant est X et le 2e mot suivant est catégorisé Y.
#                 "WDPREVTAG", #Le mot précédent est X et le 2e mot précédent est catégorisé Y.
#                ]


PREVTAG
NEXTAG
PREV1OR2TAG
NEXT1OR2TAG
PREV1OR2OR3TAG
PREVBIGRAM
NEXTBIGRAM
SURROUNDTAG

PREVWD
NEXTWD
PREV2WD
NEXT2WD
PREV1OR2WD
NEXT1OR2WD
RBIGRAM
LBIGRAM
CURWD

WDPREVTAG
WDNEXTTAG
WDAND2BFR
WDAND2AFT
WDAND2TAGBFR
WDAND2TAGAFT

Original templates :
1. The preceding (following) word is tagged z.
2. The word two before (after) is tagged z.
3. One of the two preceding (following) words is tagged z.
4. One of the three preceding (following) words is tagged z.
5. The preceding word is tagged z and the following word is tagged w.
6. The preceding (following) word is tagged z and the word two before (after) is tagged w.
7. The current word is (is not) capitalized.
8. The previous word is (is not) capitalized.


Algo :

a) We are given a small list of so called rule templates. Brill uses the following
templates in his paper:
i. change tag A to B if preceding (following) word has tag C,
ii. change tag A to B if word two before (after) has tag C,
iii. change tag A to B if one of the two preceding (following) words has
tag C,
iv. change tag A to B if one of the three preceding (following) words has
tag C,
v. change tag A to B if preceding word has tag C and following word has
tag D,
vi. change tag A to B if preceding word has tag C and word two before
has tag D,
vii. change tag A to B if following word has tag C and word two after has
tag D,
viii. change tag A to B if current (previous) word is capitalized.
(b) For each rule r, which can be generated using these templates, we compute
two statistics:
i. good(r) — number of places in the patch corpus, where the rule
matches and changes an incorrect tag into a correct one,
ii. bad(r) — number of places in the patch corpus, where the rule matches
and changes the tagging from correct to incorrect.
(c) Now we find rule rb , which maximizes good(r) − bad(r), i.e. reduces the
largest possible number of errors when applied. We save the rule and apply
it to the patch corpus. If the training corpus still contains many errors,
return to 3a.
4. The test corpus is first tagged using the unigram tagger, and then the saved
rules are applied in order.
If the test corpus was previously manually tagged, we can evaluate the performance
of the tagger.


Training Speedup: Hepple
Disallows interaction between learnt rules,
by enforcing two assumptions:
Sample independence: a state change in a sample
does not change the context of surrounding
samples
Rule commitment: there will be at most one state
change per sample
: Impressive reduction in training time, but the
quality of the results is reduced (assumptions do
not always hold)

Contextual rules can only change from a word from a tag in the lexicon to another
tag in the lexicon – unless the word is unknown (not in the
lexicon)

Il faudra tester avec des textes catégorisés mais non utilisés pour faire le lexique
Voire : pour l'apprentissage, il faut utiliser des textes catégorisés non utilisés pour le lexique
Ne garder que les catégories lexicales fermées dans lexique ?

taguer les non alphanumériques qui ne sont pas le lexique comme symbole par défaut ?

Suggérer des entrées dans le lexique quand un mot est toujours tagué pareil dans le corpus d'apprentissage?

Gérer mi- et ex- (les préfixes d'une manière générale...)
Gérer 1er
rendez-vous

Templates lexicaux :

deletepref / deletesuf		fdeletepref /fdeletesuf
addpref / addsuf		faddpref /faddsuf
haspref / hassuf		fhaspref /fhassuf
goodleft / goodright		fgoodleft / fgoodright
char			fchar

Changer le tag en X si :
- deletepref => enlever le préfixe Y donne un mot existant (Y < 4)8 : Y deletepref len(Y) X
Ex. : re deletepref 2 VNCFF 
fdeletepref => Le mot est catégorisé Z et enlever le préfixe Y donne un mot existant (Y < 4) : Z Y fdeletepref len(Y) X
Ex. : VNCFF re fdeletepref 2 SBC:sg
deletesuf => Enlever le suffixe Y donne un mot existant (Y < 4) Y deletesuf len(Y) X
Ex. : és deletesuf 2 SBC:sg
fdeletesuf => Le mot est catégorisé Z et enlever le suffixe Y donne un mot existant (Y < 4) : Z Y fdeletesuf len(Y) X
Ex. : VNCFF er fdeletesuf 2 SBC:sg
addsuf => Ajouter le suffixe Y donne un mot existant (Y < 4) : Y addsuf len(Y) X
Ex. : ir addsuf 2 VNCFF
faddsuf => Le mot est catégorisé Z et ajouter le suffixe Y donne un mot existant (Y < 4) : Z Y faddsuf len(Y) X
Ex. : ADJ:sg ir faddsuf 2 VNCFF
addpref => Ajouter le préfixe Y donne un mot existant (Y < 4) : Y addpref len(Y) X
Ex. : re addpref 2 VNCFF
faddpref => Le mot est catégorisé Z et ajouter le préfixe Y donne un mot existant (Y < 4) : Z Y faddpref len(Y) X
Ex. : SBC:sg re faddpref 2 VNCFF
haspref => Les premiers caractères du mot sont Y (Y < 4) : Y haspref len(Y) X
Ex. : pro haspref 3 SBC:sg
fhaspref => Le mot est catégorisé Z et les premiers caractères du mot sont Y (Y < 4) : Z Y fhaspref len(Y) X
Ex. : VNCFF pro fhaspref 3 SBC:sg
hassuf => Les derniers caractères du mot sont Y (Y < 4) : Y hassuf len(Y) X
Ex. : ment hassuf 4 ADV
fhassuf => Le mot est catégorisé Z et les derniers caractères du mot sont Y (Y < 4) : Z Y hassuf len(Y) X
Ex. : SBC:sg ment hassuf 4 ADV
char => Le caractère Y apparaît dans le mot : Y char X
Ex. : k char PWB
fchar => Le mot est catégorisé Z et le caractère Y apparaît dans le mot : Z Y fchar X
Ex. : SBC:sg $ char SYM
goodright => Le mot courant est placé à droite du mot Y : Y goodright X
Ex. : $ goodright CD
fgoodright => Le mot courant est tagué Z et il est placé à droite du mot Y : Z Y fgoodright X
Ex. : NN $ fgoodright CD
goodleft => le mot courant est placé à gauche du mot Y : Y goodleft X
Ex. : $ goodleft CD
fgoodleft => Le mot courant est tagué Z et il est placé à gauche du mot Y : Z Y fgoodleft X
Ex. : NN $ fgoodleft CD

Prendre tous les mots d'un corpus non lexicalisé (mais tagué), pour chaque mot mal tagué, appliquer le tag par défaut, puis tenter chacun des règles de lexicalisation : par exemple enlever la dernière lettre donne un mot dans le lexique ? Oui, on crée une règle deletepref en précisant la lettre qu'on vient d'enlever le tag principal du mot dans le lexique.
Pour chacune des règles, on l'applique à tous les mots de ce corpus non lexicalisé : si la règle ne peut pas s'appliquer sur le mot, on passe au mot suivant, sinon, si la règle s'applique et que le résultat donne un mot du dictionnaire, on écremente good, sinon si elle s'applique mais ne donne aucun mot dans le lexique, on incrément bad.
La règle qui a le plus de point (et supérieur à un minimum x) est ajoutée, avec son score, dans la liste des règles finales.
A la fin du traitement, on hiérarchise les règles par ordre décroissant et on les range dans un fichier à part.


Hahem : l'aveugle allemand vs l'allemand aveugle...

Pour les ngrams : même stemm + même tag ?


Proposition d'algo pour le training sémantique qui permette un apprentissage au fil de l'eau :
création du concept de Déclencheur (trigger)
Tou au début de l'apprentissage, on initialise un déclencheur par descripteur, qu'on lie d'une relation d'un score élevé (100 par exemple, selon l'échelle hé hé)
Pour chaque texte déjà catégorisé par un humain :
- extraire les KeyEntities
- pour chaque KeyEntity :
    - regarder si elle existe comme Trigger, si non créer ce Trigger
    - pour chaque descripteur posé sur le texte
        - regarder si une relation existe déjà entre le trigger et le descripteur, si non la créer
        - si oui, incrémenter sa valeur en fonction de la fréquence de la KeyEntity dans le texte courant
Il en résulte que des liens sont créés entre des déclencheurs et des descripteurs non réellement liés, mais on compte sur la masse et la statistique pour que ces liens soient d'impact nul dans l'utilisation future. (Y a-t-il moyen de faire le ménage autrement qu'en se lançant dans une comparaison de textes tagués ?)
Utilisation :
Lorsqu'on catégorise un nouveau texte automatiquement :
- on extrait les KeyEntities
- pour chacune, on regarde si c'est un déclencheur, si oui, on crée un descripteur pour chacune de ses relations comme descripteurs (supérieur à n ?)
- à la fin on trie les descripteurs par ordre décroissants


Lemmatisation à proprement parler :
- créer une liste de mots avec leur tag et la version lemmatisée, ex. :
mangerai VCJ:sg manger
sortais VCJ:sg sortir
portais VCJ:sg porter

(Ou utiliser certains des textes du corpus. Par exemple avec une extension spécifique...)

Boucler ensuite sur les lignes, et tester quelle transformation permet de passer du mot original à sa version lemmatisée, l'appliquer ensuite sur tout les termes, et voir le score obtenu : s'il est positif, on retient la règle.
A la fin, on classe les règles par score descendant.
Voilà pour l'apprentissage.
Ensuite, ce sont ces règles qui seront utilisées en production.
Maybe faut créer des exceptions, no sé.
Les règles seraient du type :
tag suffixe-à-enlever suffixe-à-ajouter
(avec optionnellement une mise en bas-de-casse de la première lettre ?)
 Ou bien plutôt faire un passage spécifique qui met en minuscule tous les tags x, y et z, mais pas SBP:...)
Ex. :
Mangerai VCJ:sg manger
- préprocesser pour mettre en minuscule puisque ce n'est pas un SBP
- supprimer la dernière lettre (i)
- voir si le "radical" obtenu (mangera) est contenu dans le lemme (manger) => non
- supprimer les deux dernières lettres (ai)
- voir si le "radical" obtenu "manger" est contenu dans le lemme => oui
- définir le suffixe à ajouter pour obtenir le lemme => ici "" (chaîne vide)
- définir la modification comme règle : VCJ:sg "ai" ""
- retenir la règle pour la tester
- essayer avec une lettre de plus (rai), et ainsi de suite tant qu'il reste des lettres dans le mot
- tester ensuite toutes les règles obtenues sur tous les mots du corpus, et garder celle qui obtient le meilleur score
- Faire de même sur tous les mots
- à la fin classer les règles selon leur score

Types de règles :
- enlever les majuscules parce que le tag est x
(- ne pas appliquer de règle parce que le tag est x ?)
- donner tel lemme parce que le tag est x
- appliquer telle transformation parce que le tag est x ET le mot a tel suffixe

Ecoute => écouter...
Elles/PRV:pl/elle ou il ?
ahmadinejad-car.com/PUL ?

Voir pour ne pas rester une règle déjà appliqué dans brill

Ajouter une règle pour les noms communs en début de phrase avec une cape ?

A la fin de l'apprentissage du lemmatiseur, ajouter une règle par erreur restante mot + tag => mot ?
Plutôt => ajouter les lemmatisation dans le lexique

A chaque règle positive, ajouter des règles "négatives" :
appliquer la première règle sauf si une des règles négatives s'applique ?
Pour les règles négatives, ne retenir que 100 % de good ?


========================
Apprentissage sémantique
========================
L'utilisation principale des réseaux de neurones est justement de pouvoir, à partir d'une liste de n informations, déterminer à laquelle des p classes possibles appartient cette liste. => dans le cas de la catégorisation, il faut pouvoir déterminer plusieurs classes finales...

à creuser : Support Vector Machine (SVM), Maximum Entropy Models (MEM)

First algo :
            #We do not have to attach all descriptors to all triggers
            #Here is some "algo" :
            # -if some trigger have yet a link with a current descriptor, just update this relation
            #  and mark this descriptor as "triggered", so it will no be attached to new triggers
            # -if some trigger has no relation with current descriptors, attach to it just the 
            #  descriptors not yet attached
            # TO BE OPTIMIZED

Second algo (pseudo code):
t = trigger
d = descriptor
t_d = relation between trigger t and descriptor d
cal_d = calculated descriptor (by semantical tagger)
hum_d = human tagged descriptor
cur_t = current triggers
true_positive = calculated descriptors who where in human tagged ones (true positive)
false_positive = calculated descriptors who where not in human tagged ones (false positive)
false_negative = human tagged descriptors that was not in calculated ones (false negative)

for d in true_positive:
    for t in cur_t:
        t_d += 1#increase every trigger (or just the existing ones ?)
for d in false_positive:
    for t in cur_t:
        if t_d: t_d -= 1#devaluate just the ones who have scored these descriptors
for d in false_negative:
    #we guess all the current trigger where relevants
    #because we can't do better (or don't consider yet triggered triggers ?)
    for t in cur_t:
        t_d += 1

rendre les scores proportionnels, sinon potentiellement plus un mot apparaît plus ses scores sont élevés

Supprimer les relations dès lors qu'elles sont négatives ? (Sachant qu'elles peuvent quand même en théorie redevenir positives...)
Mettre 0 comme valeur par défaut quand on attache un descripteur ? Ou sinon comment calculer cette valeur ? Utiliser le score courant me semble une bonne piste.

Ressources en ligne
-------------------

http://www.christian-faure.net/2007/05/30/introduction-au-text-mining/

Troisième algo :
# we create and ponderate all relation with a default weight (number of relations)
for text in semantical_tagged_corpus:
    for trigger in text:
        trigger.count += 1
        for validated_descriptor in text:
            #here we can maybe take number of trigger occurrences in the text
            triggertodescriptor += 1
compute pondered_weith by :
the weight of the connection / the max_weight of the trigger
*
the weight of the connection / the max_weight of the descriptor
=> this means that a connection has a 100 % weight (1.0) when it's the higher 
connection for the triiger AND the higher connection for the descriptor.
clean connections pondered_weight < 0.01

================================================
Natural Language Processing with Python - résumé
================================================

Chapitre 1
----------

**Quelques méthodes de la class nltk.text.Text**

text.concordance("string") => retourne les occurrences de la chaîne "string" dans le texte "text"

text.similar("string") => retourne les items ayant une utilisation similaire à la chaîne "string" dans le texte "text"

text.common_context(["string1", "string2"]) => retourne les contextes d'utilisation similaire aux deux chaînes "string1" et "string2"

text.dispersion_plot(["string1", "strign2"]) => crée une image représentation la position de chacun des mots dans le texte (NumPy and Matplotlib required)

text.generate() => génère un texte aléatoire reprenant les mots les plus courants du texte "text" dans leurs utilisations les plus courantes.

**Calculer la "richesse sémantique" d'un texte**

len(text) / len(set(text)) => le nombre de mots divisé par le nombre de mots différents

**Compter le nombre d'occurrences d'une graphie**

text.count("string") (reprend les méthodes de list)

**Fréquence de distribution d'un mot (frequency distribution)**

Exemple de code::

 >>> fdist = FreqDist(text)
 >>> fdist
 <FreqDist with 260819 outcomes>
 >>> vocabulary = fdist.keys() #On récupère seulement les clés, qui sont les chaînes
 >>> vocabulary[:50]
 [',', 'the', '.', 'of', 'and', 'a', 'to', ';', 'in', 'that', "'", '-',
 'his', 'it', 'I', 's', 'is', 'he', 'with', 'was', 'as', '"', 'all', 'for',
 'this', '!', 'at', 'by', 'but', 'not', '--', 'him', 'from', 'be', 'on',
 'so', 'whale', 'one', 'you', 'had', 'have', 'there', 'But', 'or', 'were',
 'now', 'which', '?', 'me', 'like']
 >>> fdist['whale']
 906

**Récupérer seulement les mots de plus de x lettres et de plus x occurrences**

sorted([w for w in set(text) if len(w) > 7 and fdist[w] > 7])

**Extraire les bigrams d'un texte**

bigram(text)

**Extraire les collocations**

text.collocations()

Chapitre 2
----------

Utilisation des corpus fournis avec ntlk.

Créer notre propre corpus::

    >>> from nltk.corpus import PlaintextCorpusReader
    >>> corpus_root = 'path/to/dir'
    >>> wordlists = PlaintextCorpusReader(corpus_root, '.*', encoding="utf-8")
    >>> wordlists.fileids()
    ['README', 'connectives', 'propernames', 'web2', 'web2a', 'words']
    >>> wordlists.words('connectives')
    ['the', 'of', 'and', 'to', 'a', 'in', 'that', 'is', ...]
    >>> wordlists.sents('connectives') #Pour avoir les phrases

On peut passer en paramètre les functions pour spliter en paragraphe, en mots et en phrases.

Par défaut, le split en paragraphe est fait avec un saut de ligne.





